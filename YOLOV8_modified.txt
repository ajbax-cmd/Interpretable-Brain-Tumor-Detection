import torch
import io
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from ultralytics.nn.modules.conv import Conv, Concat
from ultralytics.nn.modules.block import SPPF, C2f
from ultralytics.nn.modules.head import Detect, Segment
from ultralytics.utils import LOGGER
from ultralytics.utils.torch_utils import intersect_dicts



class YOLOv8mModified(nn.Module):
    def __init__(self, nc=80):
        super(YOLOv8mModified, self).__init__()
        width, depth = 0.75, 0.67
        self.backbone = nn.Sequential(
            Conv(3, int(64 * width), 3, 2),  # P1/2                     index:0
            Conv(int(64 * width), int(128 * width), 3, 2),  # P2/4      index:1
            C2f(int(128 * width), int(128 * width), int(3 * depth)),#   index:2
            Conv(int(128 * width), int(256 * width), 3, 2),  # P3/8     index:3
            C2f(int(256 * width), int(256 * width), int(6 * depth)),#   index:4
            Conv(int(256 * width), int(512 * width), 3, 2),  # P4/16    index:5
            C2f(int(512 * width), int(512 * width), int(6 * depth)),#   index:6
            Conv(int(512 * width), int(1024 * width), 3, 2),  # P5/32   index:7
            C2f(int(1024 * width), int(1024 * width), int(3 * depth)),# index:8
            SPPF(int(1024 * width), int(1024 * width), 5) #             index:9
        )
        self.head = nn.Sequential(
            nn.Upsample(scale_factor=2, mode="nearest"),                                    # index:10
            Concat(),                                                                       # index:11
            C2f(int(1024 * width) + int(512 * width), int(512 * width), int(3 * depth)),    # index:12
            nn.Upsample(scale_factor=2, mode="nearest"),                                    # index:13
            Concat(),                                                                       # index:14
            C2f(int(512 * width) + int(256 * width), int(256 * width), int(3 * depth)),     # index:15
            Conv(int(256 * width), int(256 * width), 3, 2),                                 # index:16
            Concat(),                                                                       # index:17
            C2f(int(512 * width) + int(256*  width), int(512 * width), int(3 * depth)),     # index:18
            Conv(int(512 * width), int(512 * width), 3, 2),                                 # index:19
            Concat(),                                                                       # index:20
            C2f(int(1024 * width) + int(512 * width), int(1024 * width), int(3 * depth)),   # index:21
            Detect(nc=nc, ch=(int(256 * width), int(512 * width), int(1024 * width)))       # Detect                                          
        )
        self.stride = torch.tensor([8, 16, 32])
        num_layers = len(self.backbone) + len(self.head)
        self.features = [[] for _ in range(num_layers)]  # Initialize features array that will hold extracted featurs from each layer
        self.predictions = None  # Initialize a class member for predictions

    # x= input tensor, features = 2d array with dimensions [n][k], n = num of layers, k = num of training samples
    def forward(self, batch):
        if isinstance(batch, dict):
            images = batch['img']  # Use the correct key for images
        else:
            images = batch

        tensor_outputs = []  # store each layer's output for use with Concat
        concat_layers = [None] * (len(self.backbone) + len(self.head))  # list to store layers to concat with
        # layers to concatenate with the previous layer per .yaml
        concat_layers[11] = 6
        concat_layers[14] = 4
        concat_layers[17] = 12
        concat_layers[20] = 9
        i = 0  # index

        # Forward pass through the backbone
        for layer in self.backbone:
            images = layer(images)
            tensor_outputs.append(images)
            flat_features = self.flatten_1d(images)  # flatten the batch of tensors into 1D numpy arrays
            for flat in flat_features:
                self.features[i].append(flat)
            i += 1

        # Forward pass through the head
        for layer in self.head[:-1]:
            if isinstance(layer, Concat):
                if concat_layers[i] is not None:
                    concat_inputs = [tensor_outputs[i-1], tensor_outputs[concat_layers[i]]]
                    print("tensor_output length: ", len(tensor_outputs))
                    print("i: ", i)
                    print(f"Concatenating layer indices {i-1} and {concat_layers[i]}: {tensor_outputs[-1].shape}, {tensor_outputs[concat_layers[i]].shape}")  # Debug print
                    images = layer(concat_inputs)
                    print("Image Shape from Concat: ", images.shape)
                    tensor_outputs.append(images)
                else:
                    print("Error - Concat Layer index does not match expected", i)
            else:
                images = layer(images)
                tensor_outputs.append(images)
                flat_features = self.flatten_1d(images)  # flatten the batch of tensors into 1D numpy arrays
                for flat in flat_features:
                    self.features[i].append(flat)
            i += 1

        self.predictions = self.head[-1](images)
        loss_items = []  # Placeholder for loss items
        return self.predictions, loss_items  # Return predictions and loss items

    
    def flatten_1d(self, feature_tensor):
        # Flatten the spatial dimensions and channels for each tensor in the batch
        flattened = feature_tensor.view(feature_tensor.size(0), -1)  # flatten all dimensions except batch dimension
        return [f.cpu().detach().numpy() for f in flattened]
     
    # used to load pretrained weights into model example: 
    def load(self, weights, verbose=True):
        """
        Load the weights into the model.

        Args:
            weights (dict | torch.nn.Module): The pre-trained weights to be loaded.
            verbose (bool, optional): Whether to log the transfer progress. Defaults to True.
        """
        model = weights  # torchvision models are not dicts
        csd = model.float().state_dict()  # checkpoint state_dict as FP32
        #csd = intersect_dicts(csd, self.state_dict())  # intersect
        self.load_state_dict(csd, strict=False)  # load
        if verbose:
            LOGGER.info(f"Transferred {len(csd)}/{len(self.state_dict())} items from pretrained weights")





def main():
    model = YOLOv8mModified(nc=80)
    model.load('/home/alan/Documents/YOLOv8_custom/yolov8m.pt')
    print(model)
    print("Model loaded with pretrained weights.")
    #example initialization of features array to be passed to forward_all method during training
    num_layers = len(model.backbone) + len(model.head[:-1])
    features = [[] for _ in range(num_layers)]
#main()
